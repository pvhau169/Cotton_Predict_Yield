{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#import library\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import date\n",
    "from neupy import algorithms\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "import keras\n",
    "from keras import regularizers\n",
    "import pyrenn as prn\n",
    "from keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_parameters = ['ch', 'cv', 'ExG', 'NDVI']\n",
    "plant_date = '0401'\n",
    "limit_day = 105\n",
    "interval_day = 7\n",
    "x_pred = range(interval_day, limit_day+1, interval_day)\n",
    "path = 'sigmoid_2016_spring/sigmoid_2016_spring{}.csv'\n",
    "yield_path = '2016_spring/2016_spring.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "canopy_dataframe = pd.read_excel('cotton_2016_alldata_row.xlsx', 'ALL')\n",
    "akash_data = pd.read_excel('CH.xlsx', 'Sheet1')\n",
    "index_mask = pd.read_excel('index_of_considered_samples.xlsx', 'Sheet1', header = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(802, 160)\n",
      "802\n"
     ]
    }
   ],
   "source": [
    "mask  = np.isin(canopy_dataframe['order'].values, index_mask.values) & (canopy_dataframe['Weight per row (lbs)'].values > 0)\n",
    "# print(canopy_dataframe['order'].values.shape)\n",
    "# print(index_mask.values.shape)\n",
    "canopy_dataframe = canopy_dataframe[mask]\n",
    "print(canopy_dataframe.shape)\n",
    "print(sum(mask))\n",
    "# print(canopy_dataframe.shape)\n",
    "# # print(sum(mask_CH))\n",
    "canopy_dataframe = canopy_dataframe.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_data = canopy_dataframe.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cottonYieldData(parameters):\n",
    "    x_normalization_mask = []\n",
    "    processed_data = {}\n",
    "    std = parameters['std']\n",
    "    normalization = parameters['normalization']\n",
    "    interval_day = parameters['interval_day']\n",
    "    limit_day = parameters['limit_day']\n",
    "    \n",
    "    \n",
    "    \n",
    "    plant_date = '0401'\n",
    "    file_parameters = parameters['file_parameters']\n",
    "    \n",
    "    for (i, parameter) in enumerate(file_parameters):\n",
    "    #     print(type(data[parameter]))\n",
    "        parameter_data = pd.read_csv(path.format('_'+parameter))\n",
    "        parameter_data = parameter_data.values\n",
    "        if i == 0:\n",
    "             interval_process_data = np.empty([len(parameter_data), 0])\n",
    "        if normalization:\n",
    "            x_normalization_mask.append(np.linalg.norm(parameter_data[:,:3]))\n",
    "            x_normalization_mask.append(np.linalg.norm(parameter_data[:,3:]))\n",
    "            parameter_data[:,:3] /= np.linalg.norm(parameter_data[:,:3])\n",
    "            parameter_data[:,3:] /= np.linalg.norm(parameter_data[:,3:])\n",
    "            \n",
    "        interval_process_data = np.append(interval_process_data, parameter_data, axis = 1)\n",
    "    print(interval_process_data.shape)\n",
    "    extra_parameters = data_parameters['extra_parameters']\n",
    "    for parameter in extra_parameters:\n",
    "        data = original_data[[x for x in original_data.columns if parameter in x]].copy().values.astype(float)\n",
    "        x_normalization_mask.append(np.linalg.norm(data))\n",
    "        data /= np.linalg.norm(data)\n",
    "        interval_process_data = np.append(interval_process_data, data, axis = 1)\n",
    "        \n",
    "    print(interval_process_data.shape)\n",
    "    one_hot_encoded_parameters = data_parameters['one_hot_encoded_parameters']\n",
    "     #Added one hot of irrigation\n",
    "    for parameter in one_hot_encoded_parameters:\n",
    "        irrigation = pd.DataFrame(original_data[parameter].copy())\n",
    "        irrigation_values = np.unique(irrigation)\n",
    "        irrigation[parameter] = irrigation[parameter].map(lambda x: {value:i for (i, value) in enumerate(irrigation_values)}.get(x))\n",
    "        irrigation = pd.get_dummies(irrigation[parameter])\n",
    "        interval_process_data = np.append(interval_process_data, irrigation.values, axis = 1)    \n",
    "        \n",
    "    print(interval_process_data.shape)\n",
    "#      #Added one hot of plantDate\n",
    "#     test_plant_date = pd.DataFrame(yield_data['plantDate'])\n",
    "#     test_plant_data_values = np.unique(yield_data['plantDate'].values)\n",
    "#     test_plant_date['plantDate'] = test_plant_date['plantDate'].map(lambda x: {date:i for (i, date) in enumerate(test_plant_data_values)}.get(x))\n",
    "#     test_plant_date = pd.get_dummies(test_plant_date['plantDate'])\n",
    "#     interval_process_data = np.append(interval_process_data, test_plant_date.values, axis = 1)\n",
    "    \n",
    "        \n",
    "    \n",
    "    \n",
    "    x_train = interval_process_data \n",
    "    y_train = original_data['Weight per row (lbs)'].copy().values\n",
    "#     print(y_train.shape)\n",
    "#     print(x_train.shape)\n",
    "    \n",
    "\n",
    "    y_normalization_mask = np.linalg.norm(y_train)\n",
    "    if normalization:\n",
    "        y_train /= y_normalization_mask\n",
    "\n",
    "    return(x_train, y_train, x_normalization_mask, y_normalization_mask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(model_parameters, data_parameters, data_func):\n",
    "    \n",
    "    x_data, y_data, x_normalization_mask, y_normalization_mask = data_func(data_parameters)\n",
    "    print(x_data.shape)\n",
    "    print(y_data.shape)\n",
    "    return(trainModel(x_data, y_data, x_data.shape[1], model_parameters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def akashModel(x_data, y_data, input_shape, model_parameter):\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.25, random_state = 90)\n",
    "#     x_train, x_valid, y_train, y_valid = train_test_split(x_train, y_train, test_size=0.2, random_state = 90)\n",
    "    x_train = x_train.T\n",
    "    y_train = y_train.T\n",
    "    x_test = x_test.T\n",
    "    y_test = y_test.T\n",
    "    layer_shape = model_parameters['layer']\n",
    "    kmax = model_parameters['k_max']\n",
    "    damp_const = model_parameters['dampconst']\n",
    "    damp_fac = model_parameters['dampfac']\n",
    "    E_stop = model_parameters['E_stop']\n",
    "    verbose = model_parameters['verbose']\n",
    "\n",
    "    net = prn.CreateNN(np.append(np.append(input_shape, layer_shape), 1))\n",
    "    net = prn.train_LM(x_train, y_train, net, verbose=verbose,\n",
    "    dampfac = damp_fac, dampconst = damp_const,\n",
    "    k_max=kmax, E_stop= E_stop)\n",
    "    \n",
    "    y = prn.NNOut(x_train, net)\n",
    "    y_pred = prn.NNOut(x_test, net)\n",
    "    acc_sc_chk = pd.DataFrame({'y_train': y_train.flatten(), 'y_train_pred': y.flatten()})\n",
    "    acc_sc_chk.head()\n",
    "    r2_score_1 = r2_score(acc_sc_chk.y_train, acc_sc_chk.y_train_pred)\n",
    "    print('r2 score = ', r2_score_1, '/ 1.0')\n",
    "\n",
    "    acc_sc_chk_2 = pd.DataFrame({'y_test': y_test.flatten(), 'y_test_pred': y_pred.flatten()})\n",
    "    acc_sc_chk_2.head()\n",
    "    r2_score_2 = r2_score(acc_sc_chk_2.y_test, acc_sc_chk_2.y_test_pred)\n",
    "    print('r2 score = ', r2_score_2, '/ 1.0')\n",
    "    \n",
    "    fig, ax = plt.subplots(2, 1, constrained_layout=True, figsize=(8, 6))\n",
    "    ax[0].scatter(range(len(y_train)), y_train, color = 'black', label = 'test')\n",
    "    ax[0].scatter(range(len(y)), y, color = 'red', label = 'predict')\n",
    "    ax[0].set_xlabel('observation')\n",
    "    ax[0].set_ylabel('yield')\n",
    "    ax[0].set_title('compare train and predict')\n",
    "    ax[0].legend(loc = 'upper left' )\n",
    "    \n",
    "    ax[1].scatter(range(len(y_test)), y_test, color = 'black', label = 'test')\n",
    "    ax[1].scatter(range(len(y_test)), y_pred, color = 'red', label = 'predict')\n",
    "    ax[1].set_xlabel('observation')\n",
    "    ax[1].set_ylabel('yield')\n",
    "    ax[1].set_title('compare test and predict')\n",
    "    ax[1].legend(loc = 'upper left' )\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def akashProcess(model_parameters, data_parameters, data_func):\n",
    "    x_data, y_data, x_normalization_mask, y_normalization_mask = data_func(data_parameters)\n",
    "    print(x_data.shape)\n",
    "    print(y_data.shape)\n",
    "    akashModel(x_data, y_data, x_data.shape[1], model_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(802, 57)\n",
      "(802,)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-fe66f4cd0b82>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[0mdata_func\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcottonYieldData\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m \u001b[0makashProcess\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_parameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_parameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_func\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-23-b4af081d25e7>\u001b[0m in \u001b[0;36makashProcess\u001b[1;34m(model_parameters, data_parameters, data_func)\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0makashModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_parameters\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-12-00ea520c2db7>\u001b[0m in \u001b[0;36makashModel\u001b[1;34m(x_data, y_data, input_shape, model_parameter)\u001b[0m\n\u001b[0;32m     16\u001b[0m     net = prn.train_LM(x_train, y_train, net, verbose=verbose,\n\u001b[0;32m     17\u001b[0m     \u001b[0mdampfac\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdamp_fac\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdampconst\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdamp_const\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m     k_max=kmax, E_stop= E_stop)\n\u001b[0m\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mNNOut\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnet\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pyrenn.py\u001b[0m in \u001b[0;36mtrain_LM\u001b[1;34m(P, Y, net, k_max, E_stop, dampfac, dampconst, verbose)\u001b[0m\n\u001b[0;32m    720\u001b[0m                         \u001b[1;31m#calculate scaled inverse hessian\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    721\u001b[0m                         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 722\u001b[1;33m                                 \u001b[0mG\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mJJ\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mdampfac\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meye\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'N'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#scaled inverse hessian\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    723\u001b[0m                         \u001b[1;32mexcept\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLinAlgError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    724\u001b[0m                                 \u001b[1;31m# Not invertible. Go small step in gradient direction\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\numpy\\linalg\\linalg.py\u001b[0m in \u001b[0;36minv\u001b[1;34m(a)\u001b[0m\n\u001b[0;32m    549\u001b[0m     \u001b[0msignature\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'D->D'\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0misComplexType\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m'd->d'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    550\u001b[0m     \u001b[0mextobj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_linalg_error_extobj\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_raise_linalgerror_singular\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 551\u001b[1;33m     \u001b[0mainv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_umath_linalg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msignature\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mextobj\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mextobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    552\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mwrap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mainv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult_t\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    553\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "data_parameters = {}\n",
    "model_parameters = {}\n",
    "data_parameters['normalization'] = True\n",
    "data_parameters['std'] = 10\n",
    "data_parameters['file_parameters'] = ['ch', 'cv']\n",
    "\n",
    "data_parameters['interval_day'] = 7\n",
    "data_parameters['limit_day'] = 105\n",
    "data_parameters['extra_parameters'] = ['bollvol', 'bollnum', 'bollsize']\n",
    "\n",
    "data_parameters['one_hot_encoded_parameters'] = ['irrigation']\n",
    "\n",
    "model_parameters['k_max'] = 300\n",
    "model_parameters['verbose'] = 0\n",
    "model_parameters['layer'] = [20]\n",
    "model_parameters['dampconst'] = 5\n",
    "model_parameters['dampfac'] = 0.1\n",
    "model_parameters['E_stop'] = 0.005\n",
    "\n",
    "\n",
    "#data Type\n",
    "data_func = cottonYieldData\n",
    "\n",
    "akashProcess(model_parameters, data_parameters, data_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build model that have 2 hidden layers (64, 64)\n",
    "def buildRegressionModel(model_shape, input_shape, optimizer, loss, metrics, drop_out=None, regularizer_l1=None, regularizer_l2=None, kernel_initializer = 'uniform', drop_out_rate = 0.25):\n",
    "\n",
    "    # Define a Keras sequential model\n",
    "    model = keras.Sequential()\n",
    "    # Define the first dense layer\n",
    "    for (i, size) in enumerate(model_shape):\n",
    "        if i ==0:\n",
    "            model.add(keras.layers.Dense(size, activation='relu', input_shape=[input_shape], kernel_regularizer=regularizer_l2,\n",
    "                        activity_regularizer=regularizer_l1, kernel_initializer=kernel_initializer))\n",
    "            if drop_out:\n",
    "                model.add(keras.layers.Dropout(drop_out_rate))\n",
    "        else:\n",
    "            model.add(keras.layers.Dense(size, activation='relu', kernel_regularizer=regularizer_l2, kernel_initializer=kernel_initializer,\n",
    "                        activity_regularizer=regularizer_l1))\n",
    "            if drop_out:\n",
    "                model.add(keras.layers.Dropout(drop_out_rate))\n",
    "    model.add(keras.layers.Dense(1, kernel_initializer=kernel_initializer))\n",
    "    \n",
    "    #compile model\n",
    "    model.compile(loss=loss, optimizer=optimizer, metrics=metrics)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainModel(x_data, y_data, input_shape, parameters):    \n",
    "    #Build Hypterparameter\n",
    "    sizes = parameters['sizes'] \n",
    "    shapes = parameters['shapes']\n",
    "    drop_out = parameters['drop_out'] \n",
    "    regularizer_l1 = parameters['regularizer_l1']\n",
    "    regularizer_l2 = parameters['regularizer_l2']\n",
    "    kernel_initializer = parameters['kernel_initializer']\n",
    "    drop_out_rate = parameters['drop_out_rate']\n",
    "    #Compile Hyperparameter\n",
    "    optimizer = parameters['optimizer']\n",
    "    #optimizer = keras.optimizers.RMSprop(0.001)\n",
    "    loss = parameters['loss']\n",
    "    #loss = 'mae'\n",
    "    metrics = parameters['metrics']\n",
    "\n",
    "    #Callback Hyperparameter\n",
    "    early_stopping = parameters['early_stopping']\n",
    "    if early_stopping:\n",
    "        callbacks = [early_stopping]\n",
    "    else:\n",
    "        callbacks = None\n",
    "    verbose = parameters['verbose']\n",
    "    \n",
    "    #Train Hyperparameter\n",
    "    epochs = parameters['epochs']\n",
    "    validation_split = parameters['validation_split'] \n",
    "    \n",
    "    #Test Train Split\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.25, random_state = 90)\n",
    "    x_train, x_valid, y_train, y_valid = train_test_split(x_train, y_train, test_size=0.2, random_state = 90)\n",
    "    \n",
    "    histories ={}\n",
    "    models = {}\n",
    "    #build and train model\n",
    "    for (i, size) in enumerate(sizes):\n",
    "        models[size] = buildRegressionModel(shapes[i], input_shape, optimizer, loss, metrics, drop_out = drop_out, kernel_initializer = kernel_initializer, \n",
    "                                     regularizer_l1=regularizer_l1, regularizer_l2 =regularizer_l2)\n",
    "        histories[size] = models[size].fit(x_train, y_train, epochs=epochs, validation_data=(x_valid, y_valid), \n",
    "                                  callbacks = callbacks, verbose = verbose)\n",
    "        \n",
    "        \n",
    "    #plot results\n",
    "    colors = ['red', 'blue', 'green', 'yellow']\n",
    "    plt.figure()\n",
    "    for i, size in enumerate(sizes):\n",
    "        data_history = histories[size].history\n",
    "        loss = data_history['loss']\n",
    "        val_loss = data_history['val_loss']\n",
    "        plt.plot(val_loss, color=colors[i], linestyle = '--', label = size + '_val_loss')\n",
    "        plt.plot(loss, color=colors[i], linestyle = ':', label = size +'_loss')\n",
    "    \n",
    "    plt.legend(loc = 'upper left' )\n",
    "    plt.xlabel = 'epochs'\n",
    "    plt.ylabel = 'loss'\n",
    "    \n",
    "    \n",
    "    #Show R2 scores\n",
    "    for i, size in enumerate(sizes):\n",
    "        #train score\n",
    "        pred_train = models[size].predict(x_train)\n",
    "        train_compare = pd.DataFrame({'y_train': y_train.flatten(), 'y_train_pred': pred_train.flatten()})\n",
    "        train_r2_score = r2_score(train_compare.y_train, train_compare.y_train_pred)\n",
    "        print(size + ' train r2 score = ', train_r2_score, '/ 1.0')\n",
    "        \n",
    "        #test score\n",
    "        pred_test = models[size].predict(x_test)\n",
    "        test_compare = pd.DataFrame({'y_test': y_test.flatten(), 'y_test_pred': pred_test.flatten()})\n",
    "        test_r2_score = r2_score(test_compare.y_test, test_compare.y_test_pred)\n",
    "        print(size+' test r2 score = ', test_r2_score, '/ 1.0')\n",
    "        \n",
    "    return (models, histories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(802, 57)\n",
      "(802,)\n",
      "WARNING:tensorflow:From C:\\Users\\PhanHau\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From C:\\Users\\PhanHau\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-3f29560bd33e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[0mdata_func\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcottonYieldData\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m \u001b[0mmodels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhistories\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprocess\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_parameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_func\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-11-51c314bffd86>\u001b[0m in \u001b[0;36mprocess\u001b[1;34m(model_parameters, data_parameters, data_func)\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[1;32mreturn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_parameters\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-20-acb450c40bfa>\u001b[0m in \u001b[0;36mtrainModel\u001b[1;34m(x_data, y_data, input_shape, parameters)\u001b[0m\n\u001b[0;32m     38\u001b[0m                                      regularizer_l1=regularizer_l1, regularizer_l2 =regularizer_l2)\n\u001b[0;32m     39\u001b[0m         histories[size] = models[size].fit(x_train, y_train, epochs=epochs, validation_data=(x_valid, y_valid), \n\u001b[1;32m---> 40\u001b[1;33m                                   callbacks = callbacks, verbose = verbose)\n\u001b[0m\u001b[0;32m     41\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m   1237\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1238\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1239\u001b[1;33m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[0;32m   1240\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1241\u001b[0m     def evaluate(self,\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[0;32m    194\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    195\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 196\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfit_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    197\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   3059\u001b[0m         \u001b[0mtensor_type\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdtypes_module\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_dtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3060\u001b[0m         array_vals.append(np.asarray(value,\n\u001b[1;32m-> 3061\u001b[1;33m                                      dtype=tensor_type.as_numpy_dtype))\n\u001b[0m\u001b[0;32m   3062\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3063\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\numpy\\core\\numeric.py\u001b[0m in \u001b[0;36masarray\u001b[1;34m(a, dtype, order)\u001b[0m\n\u001b[0;32m    536\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    537\u001b[0m     \"\"\"\n\u001b[1;32m--> 538\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    539\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    540\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "parameters = {}    \n",
    "#Build Hypterparameter\n",
    "parameters['sizes'] = ['default', 'medium', 'small', 'tiny']\n",
    "# parameters['sizes'] = ['default', 'medium']\n",
    "parameters['shapes'] = [[64,64,64,64], [64,64, 64], [64, 64], [64]]\n",
    "# parameters['shapes'] = [[16,16]]\n",
    "parameters['drop_out'] = False\n",
    "parameters['regularizer_l1'] = None\n",
    "parameters['regularizer_l2'] = None\n",
    "parameters['kernel_initializer'] = 'uniform'\n",
    "parameters['drop_out_rate'] = 0.001\n",
    "\n",
    "#Compile Hyperparameter\n",
    "parameters['optimizer'] = 'adam'\n",
    "# parameters['optimizer'] = keras.optimizers.RMSprop(0.001)\n",
    "parameters['loss'] = 'mse'\n",
    "#loss = 'mae'\n",
    "parameters['metrics'] = ['mse']\n",
    "\n",
    "#Callback Hyperparameter\n",
    "parameters['monitor'] = 'val_loss'\n",
    "parameters['min_delta'] = 0.01\n",
    "parameters['patience'] = 1000\n",
    "parameters['mode'] = 'min'\n",
    "parameters['verbose'] = 0\n",
    "parameters['restore_best_weights'] = True\n",
    "# parameters['early_stopping'] = EarlyStopping(monitor=parameters['monitor'], min_delta=parameters['min_delta'], patience=parameters['patience'], mode=parameters['mode'], verbose=parameters['verbose'] , restore_best_weights = parameters['restore_best_weights'])\n",
    "parameters['early_stopping'] = None\n",
    "\n",
    "#Train Hyperparameter\n",
    "parameters['epochs'] = 8000\n",
    "parameters['validation_split'] = 0.2\n",
    "\n",
    "data_parameters = {}\n",
    "data_parameters['normalization'] = True\n",
    "data_parameters['std'] = 10\n",
    "data_parameters['date_chosen'] = ''\n",
    "data_parameters['grnn'] = True\n",
    "data_parameters['interval_day'] = 7\n",
    "data_parameters['limit_day'] = 105\n",
    "data_parameters['remove_bad_yield'] = False\n",
    "data_parameters['file_parameters'] = ['ch', 'cv']\n",
    "data_parameters['extra_parameters'] = ['bollvol', 'bollnum', 'bollsize']\n",
    "data_parameters['one_hot_encoded_parameters'] = ['irrigation']\n",
    "\n",
    "#data Type\n",
    "data_func = cottonYieldData\n",
    "\n",
    "models, histories = process(parameters, data_parameters, data_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.4 64-bit ('base': conda)",
   "language": "python",
   "name": "python37464bitbasecondae46f33a317cf44a38697b7f892796b3f"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
